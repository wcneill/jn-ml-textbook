{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Looking at the Math Behind a Neural Network:\n",
    "Here is a primer on the basic linear algebra that goes on under the hood. This particular notebook is not meant to stand alone. This is because there exist vast resources that dive deep into the mathematics behind neural networks. \n",
    "\n",
    "This particular notebook should be treated as a way to experiment with some basic code as you read along more in-depth material. I suggest taking a look at the excellent, free online book found here:\n",
    "\n",
    "http://neuralnetworksanddeeplearning.com/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def activation(x):\n",
    "    \"\"\"\n",
    "    sigmoid activation function on x\n",
    "    \"\"\"\n",
    "    return 1/(1 + torch.exp(-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single Layer Network:\n",
    "A vanilla, fully-connected network is nothing but a set of matrices representing the weights and biases of the network. \n",
    "\n",
    "Each layer of a fully connected neural network is a stack of nodes. A given node in layer $l$, will recieve the output of every node in layer $l-1$. Each input to this single node will have a weight assigned to it. The node itself will also have a bias. \n",
    "\n",
    "These weights and biases form the network, and they are what we will later learn to train through back-propagation. For now, here is a simplistic look at how feedforward works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(7)\n",
    "features = torch.randn((1, 5))\n",
    "weights = torch.randn_like(features)\n",
    "bias = torch.randn((1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. A set of inputs to the network called `features` is created. Here, `features` is a row vector.\n",
    "2. A single 'layer' of weights and bias are randomly initialized. \n",
    "3. We perform matrix-vector multiplication between the features and the layer weights.\n",
    "4. We feed the result of the matrix-vector multiplication into an 'activation' function which squishes the output to a desired range, usually [0, 1]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.6104, 0.5914, 0.5341, 0.5738, 0.6375],\n",
       "        [0.4047, 0.5095, 0.7836, 0.6050, 0.2680],\n",
       "        [0.3706, 0.4952, 0.8153, 0.6103, 0.2184],\n",
       "        [0.7883, 0.6713, 0.2581, 0.5408, 0.8995],\n",
       "        [0.2323, 0.4296, 0.9169, 0.6344, 0.0740]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = activation(torch.mm(features.T, weights) + bias)\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In mathematical notation, we are simply computing the equation of a hyper-plane (a line generalized to higher dimensions):\n",
    "\n",
    "\n",
    "$$\n",
    "y = m_1x_1 + m_2x_2 + \\dots m_nx_n + b = \\sum_{i = 0}^n m_ix_i + b \n",
    "$$\n",
    "$$\n",
    "\\text{output} = \\sigma (y)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 'Deep' Neural Networks:\n",
    "A deep neural network is one that has more than a single hidden layer. Below, we take a look at an example of a feed-forward network with 2 hidden layers.\n",
    "\n",
    "This time our weights are actual 2-dimensional matrices rather than simple row or column vectors. Each element of a weight matrix can be indexed as follows:\n",
    "\n",
    "$$ w^l_{i, j}$$\n",
    "\n",
    "Where the specific indeces can be described as:\n",
    "\n",
    "\\begin{align}\n",
    "l& \\text{ - Denotes layer $l$, the current layer.} \\\\\n",
    "i& \\text{ - Denotes input from the $i$th node in layer $l$.} \\\\\n",
    "j& \\text{ - Denotes the $j$th node in the previous layer $l - 1$.}\n",
    "\\end{align}\n",
    "\n",
    "Therefore $w^l_{i, j}$ represents the weight of the input from neuron $j$ in the last layer to neuron $i$ in the current layer. \n",
    "\n",
    "Below is a functional example in code, but I highly encourage that you read the linked book for a deeper understanding. It has been written so well, it is almost pointless for me to write more here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3171]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Generate some data\n",
    "torch.manual_seed(7) # Set the random seed so things are predictable\n",
    "\n",
    "# Features are 3 random normal variables\n",
    "features = torch.randn((1, 3))\n",
    "\n",
    "# Define the size of each layer in our network\n",
    "n_input = features.shape[1]     # Number of input units, must match number of input features\n",
    "n_hidden = 2                    # Number of hidden units \n",
    "n_output = 1                    # Number of output units\n",
    "\n",
    "# Weights for inputs to hidden layer\n",
    "W1 = torch.randn(n_input, n_hidden)\n",
    "# Weights for hidden layer to output layer\n",
    "W2 = torch.randn(n_hidden, n_output)\n",
    "\n",
    "# and bias terms for hidden and output layers\n",
    "B1 = torch.randn((1, n_hidden))\n",
    "B2 = torch.randn((1, n_output))\n",
    "\n",
    "# feed forward:\n",
    "a1 = activation(torch.mm(features, W1) + B1)\n",
    "output = activation(torch.mm(a1, W2) + B2)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
